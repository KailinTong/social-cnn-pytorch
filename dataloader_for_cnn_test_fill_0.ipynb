{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataPreprocessorForCNN():\n",
    "    def __init__(self, input_seq_length=5, pred_seq_length=5, datasets=[i for i in range(37)], test_data_sets = [2], dev_ratio_to_test_set = 0.5, forcePreProcess=False, augmentation=False):\n",
    "        '''\n",
    "        Initializer function for the CustomDataSetForCNN class\n",
    "        params:\n",
    "        input_seq_length : input sequence length to be considered\n",
    "        output_seq_length : output sequence length to be predicted\n",
    "        datasets : The indices of the datasets to use\n",
    "        test_data_sets : The indices of the test sets from datasets\n",
    "        dev_ratio_to_test_set : ratio of the validation set size to the test set size\n",
    "        forcePreProcess : Flag to forcefully preprocess the data again from csv files\n",
    "        '''\n",
    "        # List of data directories where raw data resides\n",
    "        self.data_paths = ['./data/train/raw/biwi/biwi_hotel.txt', './data/train/raw/crowds/arxiepiskopi1.txt',\n",
    "                          './data/train/raw/crowds/crowds_zara02.txt', './data/train/raw/crowds/crowds_zara03.txt',\n",
    "                          './data/train/raw/crowds/students001.txt', './data/train/raw/crowds/students003.txt', \n",
    "                          './data/train/raw/stanford/bookstore_0.txt',\n",
    "                          './data/train/raw/stanford/bookstore_1.txt', './data/train/raw/stanford/bookstore_2.txt',\n",
    "                          './data/train/raw/stanford/bookstore_3.txt', './data/train/raw/stanford/coupa_3.txt',\n",
    "                          './data/train/raw/stanford/deathCircle_0.txt', './data/train/raw/stanford/deathCircle_1.txt',\n",
    "                          './data/train/raw/stanford/deathCircle_2.txt', './data/train/raw/stanford/deathCircle_3.txt',\n",
    "                          './data/train/raw/stanford/deathCircle_4.txt', './data/train/raw/stanford/gates_0.txt',\n",
    "                          './data/train/raw/stanford/gates_1.txt', './data/train/raw/stanford/gates_3.txt',\n",
    "                          './data/train/raw/stanford/gates_4.txt', './data/train/raw/stanford/gates_5.txt',\n",
    "                          './data/train/raw/stanford/gates_6.txt', './data/train/raw/stanford/gates_7.txt',\n",
    "                          './data/train/raw/stanford/gates_8.txt', './data/train/raw/stanford/hyang_4.txt',\n",
    "                          './data/train/raw/stanford/hyang_5.txt', './data/train/raw/stanford/hyang_6.txt',\n",
    "                          './data/train/raw/stanford/hyang_7.txt', './data/train/raw/stanford/hyang_9.txt',\n",
    "                          './data/train/raw/stanford/nexus_0.txt', './data/train/raw/stanford/nexus_1.txt',\n",
    "                          './data/train/raw/stanford/nexus_2.txt', './data/train/raw/stanford/nexus_3.txt',\n",
    "                          './data/train/raw/stanford/nexus_4.txt', './data/train/raw/stanford/nexus_7.txt',\n",
    "                          './data/train/raw/stanford/nexus_8.txt', './data/train/raw/stanford/nexus_9.txt']\n",
    "        train_datasets = datasets\n",
    "        for dataset in test_data_sets:\n",
    "            train_datasets.remove(dataset)\n",
    "        self.train_data_paths = [self.data_paths[x] for x in train_datasets]\n",
    "        self.test_data_paths = [self.data_paths[x] for x in test_data_sets]\n",
    "        print(\"Using the following dataset(s) as test set\")\n",
    "        print(self.test_data_paths)\n",
    "        \n",
    "        # Number of datasets\n",
    "        self.numDatasets = len(self.data_paths)\n",
    "        \n",
    "        # Data directory where the pre-processed pickle file resides\n",
    "        self.data_dir = './data/train/processed'\n",
    "        \n",
    "        # Store the arguments\n",
    "        self.input_seq_length = input_seq_length\n",
    "        self.pred_seq_length = pred_seq_length\n",
    "        \n",
    "        # Validation arguments\n",
    "        self.dev_ratio = dev_ratio_to_test_set\n",
    "        \n",
    "        # Buffer for storing raw data.\n",
    "        self.raw_data_train = []\n",
    "        self.raw_data_test = []\n",
    "        # Buffer for storing processed data.\n",
    "        self.processed_input_output_pairs_train = []\n",
    "        self.processed_input_output_pairs_test = []\n",
    "        \n",
    "        # Scale Factor for x and y (computed in self.process())\n",
    "        self.scale_factor_x = None\n",
    "        self.scale_factor_y = None\n",
    "        \n",
    "        # Data augmentation flag\n",
    "        self.augmentation = augmentation\n",
    "        # Rotation increment (deg) for data augmentation (only valid if augmentation is True)\n",
    "        self.rot_deg_increment = 120\n",
    "        # How many pedestrian permutations to consider (only valid if augmentation is True)\n",
    "        self.permutations = 4\n",
    "        \n",
    "        # Define the path in which the process data would be stored\n",
    "        self.processed_train_data_file = os.path.join(self.data_dir, \"trajectories_cnn_train.cpkl\")\n",
    "        self.processed_dev_data_file = os.path.join(self.data_dir, \"trajectories_cnn_dev.cpkl\")\n",
    "        self.processed_test_data_file = os.path.join(self.data_dir, \"trajectories_cnn_test.cpkl\")\n",
    "        \n",
    "        # If the file doesn't exist or forcePreProcess is true\n",
    "        if not(os.path.exists(self.processed_train_data_file)) or not(os.path.exists(self.processed_dev_data_file)) or not(os.path.exists(self.processed_test_data_file)) or forcePreProcess:\n",
    "            print(\"============ Normalizing raw data (after rotation data augmentation) ============\")\n",
    "            print(\"--> Finding max coordinate values for train data\")\n",
    "            x_max_train, x_min_train, y_max_train, y_min_train = self.find_max_coordinates(self.train_data_paths, self.raw_data_train)\n",
    "            print(\"--> Finding max coordinate values for test data\")\n",
    "            x_max_test, x_min_test, y_max_test, y_min_test = self.find_max_coordinates(self.test_data_paths, self.raw_data_test)\n",
    "            x_max_global, y_max_global = max([x_max_train, x_max_test]), max([y_max_train, y_max_test])\n",
    "            x_min_global, y_min_global = min([x_min_train, x_min_test]), min([y_min_train, y_min_test])\n",
    "            self.scale_factor_x = (x_max_global - x_min_global)/(1 + 1)\n",
    "            self.scale_factor_y = (y_max_global - y_min_global)/(1 + 1)\n",
    "            print(\"--> Normalizing train data\")\n",
    "            self.normalize(self.raw_data_train, x_max_global, x_min_global, y_max_global, y_min_global)\n",
    "            print(\"--> Normalizing test data\")\n",
    "            self.normalize(self.raw_data_test, x_max_global, x_min_global, y_max_global, y_min_global)\n",
    "            print(\"============ Creating pre-processed training data for CNN ============\")\n",
    "            self.preprocess(self.raw_data_train, self.processed_input_output_pairs_train, self.processed_train_data_file)\n",
    "            print(\"============ Creating pre-processed dev & test data for CNN ============\")\n",
    "            self.preprocess(self.raw_data_test, self.processed_input_output_pairs_test, self.processed_test_data_file, self.dev_ratio, self.processed_dev_data_file)\n",
    "            \n",
    "    def find_max_coordinates(self, data_paths, raw_data_buffer):\n",
    "        if self.augmentation:\n",
    "            print('--> Data Augmentation: Rotation (by ' + str(self.rot_deg_increment) + ' deg incrementally up to 360 deg)')\n",
    "        for path in data_paths:\n",
    "            # Load data from txt file.\n",
    "            txtfile = open(path, 'r')\n",
    "            lines = txtfile.read().splitlines()\n",
    "            data = [line.split() for line in lines]\n",
    "            data = np.transpose(sorted(data, key=lambda line: int(line[0]))).astype(float)\n",
    "            raw_data_buffer.append(data)            \n",
    "            if self.augmentation:\n",
    "                # Rotate data by deg_increment deg sequentially for data augmentation (only rotation is considered here)\n",
    "                deg_increment_int = int(self.rot_deg_increment)\n",
    "                for deg in range(deg_increment_int, 360, deg_increment_int):\n",
    "                    data_rotated = np.zeros_like(data)\n",
    "                    rad = np.radians(deg)\n",
    "                    c, s = np.cos(rad), np.sin(rad)\n",
    "                    Rot = np.array(((c,-s), (s, c)))\n",
    "                    for ii in range(data.shape[1]):\n",
    "                        data_rotated[0:2, ii] = data[0:2, ii]\n",
    "                        data_rotated[2:, ii] = np.dot(Rot, data[2:, ii])\n",
    "                    raw_data_buffer.append(data_rotated)\n",
    "        # Find x_max, x_min, y_max, y_min across all the data in data_paths.\n",
    "        x_max_global, x_min_global, y_max_global, y_min_global = -1000, 1000, -1000, 1000\n",
    "        for data in raw_data_buffer:\n",
    "            x = data[2,:]\n",
    "            x_min, x_max = min(x), max(x)\n",
    "            if x_min < x_min_global:\n",
    "                x_min_global = x_min\n",
    "            if x_max > x_max_global:\n",
    "                x_max_global = x_max\n",
    "            y = data[3,:]\n",
    "            y_min, y_max = min(y), max(y)\n",
    "            if y_min < y_min_global:\n",
    "                y_min_global = y_min\n",
    "            if y_max > y_max_global:\n",
    "                y_max_global = y_max\n",
    "        return x_max_global, x_min_global, y_max_global, y_min_global\n",
    "        \n",
    "    def normalize(self, raw_data_buffer, x_max_global, x_min_global, y_max_global, y_min_global):\n",
    "        # Normalize all the data in this buffer to range from -1 to 1.\n",
    "        for data in raw_data_buffer:\n",
    "            x = data[2,:]\n",
    "            x = (1 + 1)*(x - x_min_global)/(x_max_global - x_min_global)\n",
    "            x = x - 1.0\n",
    "            for jj in range(len(x)):\n",
    "                if abs(x[jj]) < 0.0001:\n",
    "                    data[2,jj] = 0.0\n",
    "                else:\n",
    "                    data[2,jj] = x[jj] \n",
    "            y = data[3,:]\n",
    "            y = (1 + 1)*(y - y_min_global)/(y_max_global - y_min_global)\n",
    "            y = y - 1.0\n",
    "            for jj in range(len(y)):\n",
    "                if abs(y[jj]) < 0.0001:\n",
    "                    data[3,jj] = 0.0\n",
    "                else:\n",
    "                    data[3,jj] = y[jj]\n",
    "        '''# Sanity check.\n",
    "        # Find x_max, x_min, y_max, y_min in this raw_data_buffer\n",
    "        x_max_buffer, x_min_buffer, y_max_buffer, y_min_buffer = -1000, 1000, -1000, 1000\n",
    "        for data in raw_data_buffer:\n",
    "            x = data[2,:]\n",
    "            x_min, x_max = min(x), max(x)\n",
    "            if x_min < x_min_buffer:\n",
    "                x_min_buffer = x_min\n",
    "            if x_max > x_max_buffer:\n",
    "                x_max_buffer = x_max\n",
    "            y = data[3,:]\n",
    "            y_min, y_max = min(y), max(y)\n",
    "            if y_min < y_min_buffer:\n",
    "                y_min_buffer = y_min\n",
    "            if y_max > y_max_buffer:\n",
    "                y_max_buffer = y_max\n",
    "        print(x_min_buffer, x_max_buffer)\n",
    "        print(y_min_buffer, y_max_buffer)\n",
    "        '''\n",
    "    def preprocess(self, raw_data_buffer, processed_input_output_pairs, processed_data_file, dev_ratio=0., processed_data_file_2=None):\n",
    "        random.seed(1) # Random seed for pedestrian permutation and data shuffling\n",
    "        for data in raw_data_buffer:\n",
    "            # Frame IDs of the frames in the current dataset\n",
    "            frameList = np.unique(data[0, :].astype(int)).tolist()\n",
    "            #print(frameList)\n",
    "            numFrames = len(frameList)\n",
    "            \n",
    "            # Frame ID increment for this dataset.\n",
    "            frame_increment = np.min(np.array(frameList[1:-1]) - np.array(frameList[0:-2]))\n",
    "            \n",
    "            # For this dataset check which pedestrians exist in each frame.\n",
    "            pedsInFrameList = []\n",
    "            pedsPosInFrameList = []\n",
    "            for ind, frame in enumerate(frameList):\n",
    "                # For this frame check the pedestrian IDs.\n",
    "                pedsInFrame = data[:, data[0, :].astype(int) == frame]\n",
    "                pedsList = pedsInFrame[1, :].astype(int).tolist()\n",
    "                pedsInFrameList.append(pedsList)\n",
    "                # Position information for each pedestrian.\n",
    "                pedsPos = []\n",
    "                for ped in pedsList:\n",
    "                    # Extract x and y positions\n",
    "                    current_x = pedsInFrame[2, pedsInFrame[1, :].astype(int) == ped][0]\n",
    "                    current_y = pedsInFrame[3, pedsInFrame[1, :].astype(int) == ped][0]\n",
    "                    pedsPos.extend([current_x, current_y])\n",
    "                    if (current_x == 0.0 and current_y == 0.0):\n",
    "                        print('[WARNING] There exists a pedestrian at coordinate [0.0, 0.0]')\n",
    "                pedsPosInFrameList.append(pedsPos)\n",
    "            # Go over the frames in this data again to extract data.\n",
    "            ind = 0\n",
    "            while ind < len(frameList) - (self.input_seq_length + self.pred_seq_length):\n",
    "                # Check if this sequence contains consecutive frames. Otherwise skip this sequence.\n",
    "                if not frameList[ind + self.input_seq_length + self.pred_seq_length - 1] - frameList[ind] == (self.input_seq_length + self.pred_seq_length - 1)*frame_increment:\n",
    "                    ind += 1\n",
    "                    continue\n",
    "                # List of pedestirans in this sequence.\n",
    "                pedsList = np.unique(np.concatenate(pedsInFrameList[ind : ind + self.input_seq_length + self.pred_seq_length])).tolist()\n",
    "                # Print the Frame numbers and pedestrian IDs in this sequence for sanity check.\n",
    "                # print(str(int(self.input_seq_length + self.pred_seq_length)) + ' frames starting from Frame ' + str(int(frameList[ind])) +  ' contain pedestrians ' + str(pedsList))\n",
    "                # Initialize numpy arrays for input-output pair\n",
    "                data_input = np.zeros((2*len(pedsList), self.input_seq_length))\n",
    "                data_output = np.zeros((2*len(pedsList), self.pred_seq_length))\n",
    "                for ii in range(self.input_seq_length):\n",
    "                    for jj in range(len(pedsList)):\n",
    "                        if pedsList[jj] in pedsInFrameList[ind + ii]:\n",
    "                            datum_index = pedsInFrameList[ind + ii].index(pedsList[jj])\n",
    "                            data_input[2*jj:2*(jj + 1), ii] = np.array(pedsPosInFrameList[ind + ii][2*datum_index:2*(datum_index + 1)])\n",
    "                for ii in range(self.pred_seq_length):\n",
    "                    for jj in range(len(pedsList)):\n",
    "                        if pedsList[jj] in pedsInFrameList[ind + self.input_seq_length + ii]:\n",
    "                            datum_index = pedsInFrameList[ind + self.input_seq_length + ii].index(pedsList[jj])\n",
    "                            data_output[2*jj:2*(jj + 1), ii] = np.array(pedsPosInFrameList[ind + self.input_seq_length + ii][2*datum_index:2*(datum_index + 1)])\n",
    "                processed_pair = (torch.from_numpy(data_input), torch.from_numpy(data_output))\n",
    "                processed_input_output_pairs.append(processed_pair)\n",
    "                ind += self.input_seq_length + self.pred_seq_length\n",
    "        print('--> Data Size: ' + str(len(processed_input_output_pairs)))\n",
    "        if self.augmentation:\n",
    "            # Perform data augmentation\n",
    "            self.augment_flip(processed_input_output_pairs)\n",
    "            self.augment_permute(processed_input_output_pairs)\n",
    "        else:\n",
    "            print('--> Skipping data augmentation')\n",
    "        # Shuffle data.\n",
    "        print('--> Shuffling all data before saving')\n",
    "        random.shuffle(processed_input_output_pairs)\n",
    "        if dev_ratio != 0.:\n",
    "            # Split data into dev and test sets.\n",
    "            dev_size = int(len(processed_input_output_pairs)*dev_ratio)\n",
    "            processed_dev_set = processed_input_output_pairs[:dev_size]\n",
    "            processed_test_set = processed_input_output_pairs[dev_size:]\n",
    "            print('--> Dumping dev data with size ' + str(len(processed_dev_set)) + ' to pickle file')\n",
    "            f_dev = open(processed_data_file_2, 'wb')\n",
    "            pickle.dump(processed_dev_set, f_dev, protocol=2)\n",
    "            f_dev.close()\n",
    "            print('--> Dumping test data with size ' + str(len(processed_test_set)) + ' to pickle file')\n",
    "            f_test = open(processed_data_file, 'wb')\n",
    "            pickle.dump(processed_test_set, f_test, protocol=2)\n",
    "            f_test.close()\n",
    "            # Clear buffer\n",
    "            raw_data_buffer = []\n",
    "            processed_input_output_pairs = []\n",
    "        else:\n",
    "            assert(processed_data_file_2 == None)\n",
    "            processed_train_set = processed_input_output_pairs\n",
    "            print('--> Dumping train data with size ' + str(len(processed_train_set)) + ' to pickle file')\n",
    "            f_train = open(processed_data_file, 'wb')\n",
    "            pickle.dump(processed_train_set, f_train, protocol=2)\n",
    "            f_train.close()\n",
    "            # Clear buffer\n",
    "            raw_data_buffer = []\n",
    "            processed_input_output_pairs = []\n",
    "    \n",
    "    def augment_flip(self, processed_input_output_pairs):\n",
    "        print('--> Data Augmentation: Y Flip')\n",
    "        augmented_input_output_pairs = []\n",
    "        for processed_input_output_pair in tqdm(processed_input_output_pairs):\n",
    "            data_input, data_output = processed_input_output_pair[0].numpy(), processed_input_output_pair[1].numpy()\n",
    "            num_peds = int(data_input.shape[0]/2)\n",
    "            # Flip y\n",
    "            data_input_yflipped = np.zeros_like(data_input)\n",
    "            data_output_yflipped = np.zeros_like(data_output)\n",
    "            for kk in range(num_peds):\n",
    "                data_input_yflipped[2*kk, :] = data_input[2*kk, :]\n",
    "                data_input_yflipped[2*kk+1, :] = -1*data_input[2*kk+1, :]\n",
    "                data_output_yflipped[2*kk, :] = data_output[2*kk, :]\n",
    "                data_output_yflipped[2*kk+1, :] = -1*data_output[2*kk+1, :]\n",
    "            processed_pair_yflipped = (torch.from_numpy(data_input_yflipped), torch.from_numpy(data_output_yflipped))\n",
    "            augmented_input_output_pairs.append(processed_pair_yflipped)\n",
    "        processed_input_output_pairs.extend(augmented_input_output_pairs)\n",
    "        print('--> Augmented Data Size: ' + str(len(processed_input_output_pairs)))\n",
    "        \n",
    "    def augment_permute(self, processed_input_output_pairs):\n",
    "        # Specify how many pedestrian permutations to consider per input-output pair\n",
    "        print('--> Data Augmentation: Pedestrian Permutation (' + str(self.permutations) + ' random permutations per input-output pair)')\n",
    "        augmented_input_output_pairs = []\n",
    "        for processed_input_output_pair in tqdm(processed_input_output_pairs):\n",
    "            data_input, data_output = processed_input_output_pair[0].numpy(), processed_input_output_pair[1].numpy()\n",
    "            num_peds = int(data_input.shape[0]/2)\n",
    "            for ii in range(self.permutations):\n",
    "                perm = np.random.permutation(num_peds)\n",
    "                data_input_permuted = np.zeros_like(data_input)\n",
    "                data_output_permuted = np.zeros_like(data_output)\n",
    "                for jj in range(len(perm)):\n",
    "                    data_input_permuted[2*jj:2*(jj+1), :] = data_input[2*perm[jj]:2*(perm[jj]+1), :]\n",
    "                    data_output_permuted[2*jj:2*(jj+1), :] = data_output[2*perm[jj]:2*(perm[jj]+1), :]\n",
    "                processed_pair_permuted = (torch.from_numpy(data_input_permuted), torch.from_numpy(data_output_permuted))\n",
    "                augmented_input_output_pairs.append(processed_pair_permuted)\n",
    "        processed_input_output_pairs.extend(augmented_input_output_pairs)\n",
    "        print('--> Augmented Data Size: ' + str(len(processed_input_output_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following dataset(s) as test set\n",
      "['./data/train/raw/crowds/crowds_zara02.txt', './data/train/raw/crowds/crowds_zara03.txt', './data/train/raw/crowds/students001.txt']\n",
      "============ Normalizing raw data (after rotation data augmentation) ============\n",
      "--> Finding max coordinate values for train data\n",
      "--> Data Augmentation: Rotation (by 120 deg incrementally up to 360 deg)\n",
      "--> Finding max coordinate values for test data\n",
      "--> Data Augmentation: Rotation (by 120 deg incrementally up to 360 deg)\n",
      "--> Normalizing train data\n",
      "--> Normalizing test data\n",
      "============ Creating pre-processed training data for CNN ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 3057/6513 [00:00<00:00, 15428.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Data Size: 6513\n",
      "--> Data Augmentation: Y Flip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6513/6513 [00:00<00:00, 17081.55it/s]\n",
      "  4%|▎         | 458/13026 [00:00<00:02, 4575.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Augmented Data Size: 13026\n",
      "--> Data Augmentation: Pedestrian Permutation (4 random permutations per input-output pair)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13026/13026 [00:02<00:00, 5454.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Augmented Data Size: 65130\n",
      "--> Shuffling all data before saving\n",
      "--> Dumping train data with size 65130 to pickle file\n",
      "============ Creating pre-processed dev & test data for CNN ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654/654 [00:00<00:00, 12429.20it/s]\n",
      " 39%|███▉      | 511/1308 [00:00<00:00, 5103.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Data Size: 654\n",
      "--> Data Augmentation: Y Flip\n",
      "--> Augmented Data Size: 1308\n",
      "--> Data Augmentation: Pedestrian Permutation (4 random permutations per input-output pair)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1308/1308 [00:00<00:00, 4074.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Augmented Data Size: 6540\n",
      "--> Shuffling all data before saving\n",
      "--> Dumping dev data with size 3270 to pickle file\n",
      "--> Dumping test data with size 3270 to pickle file\n"
     ]
    }
   ],
   "source": [
    "processed = CustomDataPreprocessorForCNN(forcePreProcess=True, test_data_sets=[2,3,4], augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.1025370214826"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.scale_factor_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.6984739782018"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.scale_factor_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open(processed.processed_train_data_file, 'rb')\n",
    "dev_file = open(processed.processed_dev_data_file, 'rb')\n",
    "test_file = open(processed.processed_test_data_file, 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/train/processed/trajectories_cnn_train.cpkl'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.processed_train_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(train_file)\n",
    "dev = pickle.load(dev_file)\n",
    "test = pickle.load(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65130"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3270"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3270"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetForCNN(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.file = open(self.file_path, 'rb')\n",
    "        self.data = pickle.load(self.file)\n",
    "        self.file.close()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDatasetForCNN(processed.processed_train_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_set.__getitem__(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3975, -0.3916, -0.3865, -0.3810, -0.3762],\n",
       "        [ 0.7377,  0.7441,  0.7520,  0.7580,  0.7640],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.0000,  0.0000,  0.1971,  0.1971,  0.1977],\n",
       "          [-0.0000, -0.0000,  0.0945,  0.0945,  0.0956],\n",
       "          [ 0.0816,  0.0864,  0.0896,  0.0940,  0.0978],\n",
       "          [ 0.2398,  0.2488,  0.2574,  0.2671,  0.2775],\n",
       "          [ 0.0391,  0.0394,  0.0360,  0.0338,  0.0302],\n",
       "          [ 0.1710,  0.1769,  0.1862,  0.1937,  0.2012],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000, -0.2043, -0.2043, -0.2043],\n",
       "          [-0.0000, -0.0000,  0.3170,  0.3170,  0.3170],\n",
       "          [-0.1968, -0.1962, -0.1948, -0.1940, -0.1932],\n",
       "          [ 0.2435,  0.2446,  0.2460,  0.2463,  0.2465],\n",
       "          [-0.1853, -0.1853, -0.1840, -0.1840, -0.1840],\n",
       "          [ 0.2888,  0.2888,  0.2879,  0.2879,  0.2879],\n",
       "          [ 0.1601,  0.1594,  0.1587,  0.1583,  0.1570],\n",
       "          [-0.0861, -0.0913, -0.0977, -0.1065, -0.1148],\n",
       "          [-0.2047, -0.2037, -0.2041, -0.2022, -0.2000],\n",
       "          [ 0.1933,  0.2003,  0.2062,  0.2110,  0.2165],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
       "          [ 0.2251,  0.2275,  0.2293,  0.2314,  0.2338],\n",
       "          [ 0.0394,  0.0440,  0.0488,  0.0528,  0.0573],\n",
       "          [ 0.0000,  0.0000,  0.0000, -0.2046, -0.2016],\n",
       "          [-0.0000, -0.0000, -0.0000, -0.3037, -0.2980],\n",
       "          [-0.1404, -0.1404, -0.1358, -0.1342, -0.1342],\n",
       "          [ 0.4096,  0.4096,  0.4014,  0.4019,  0.4019],\n",
       "          [-0.1609, -0.1609, -0.1549, -0.1539, -0.1544],\n",
       "          [ 0.4063,  0.4063,  0.4097,  0.4090,  0.4082]]], dtype=torch.float64),\n",
       " tensor([[[ 0.1987,  0.1987,  0.1993,  0.2006,  0.2012],\n",
       "          [ 0.0950,  0.0950,  0.0961,  0.0953,  0.0964],\n",
       "          [ 0.1012,  0.1050,  0.1093,  0.1127,  0.1178],\n",
       "          [ 0.2879,  0.2983,  0.3084,  0.3188,  0.3284],\n",
       "          [ 0.0279,  0.0256,  0.0220,  0.0195,  0.0171],\n",
       "          [ 0.2087,  0.2162,  0.2245,  0.2321,  0.2393],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0661,  0.0675],\n",
       "          [-0.0000, -0.0000, -0.0000,  0.5720,  0.5655],\n",
       "          [-0.2043, -0.2043, -0.2043, -0.2043, -0.2043],\n",
       "          [ 0.3170,  0.3170,  0.3170,  0.3170,  0.3170],\n",
       "          [-0.1921, -0.1916, -0.1914, -0.1910, -0.1909],\n",
       "          [ 0.2466,  0.2462,  0.2401,  0.2323,  0.2266],\n",
       "          [-0.1834, -0.1829, -0.1824, -0.1818, -0.1813],\n",
       "          [ 0.2891,  0.2887,  0.2884,  0.2895,  0.2892],\n",
       "          [ 0.1562,  0.1562,  0.1547,  0.1540,  0.1533],\n",
       "          [-0.1222, -0.1313, -0.1394, -0.1472, -0.1558],\n",
       "          [-0.1979, -0.1989, -0.1995, -0.2008, -0.2024],\n",
       "          [ 0.2219,  0.2233,  0.2221,  0.2230,  0.2225],\n",
       "          [ 0.0000,  0.0698,  0.0714,  0.0714,  0.0680],\n",
       "          [-0.0000, -0.3524, -0.3459, -0.3402, -0.3290],\n",
       "          [ 0.2357,  0.2379,  0.2405,  0.2429,  0.2446],\n",
       "          [ 0.0622,  0.0664,  0.0701,  0.0746,  0.0796],\n",
       "          [-0.1992, -0.1970, -0.1928, -0.1877, -0.1832],\n",
       "          [-0.2875, -0.2775, -0.2696, -0.2611, -0.2526],\n",
       "          [-0.1342, -0.1342, -0.1342, -0.1342, -0.1336],\n",
       "          [ 0.4019,  0.4019,  0.4019,  0.4019,  0.4030],\n",
       "          [-0.1550, -0.1556, -0.1551, -0.1551, -0.1557],\n",
       "          [ 0.4071,  0.4059,  0.4041,  0.4041,  0.4030]]], dtype=torch.float64)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_set.__getitem__(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65130"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
