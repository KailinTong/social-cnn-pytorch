{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataPreprocessorForCNN():\n",
    "    def __init__(self, input_seq_length=5, pred_seq_length=5, datasets=[i for i in range(37)], test_data_sets = [2], dev_ratio_to_test_set = 0.1, forcePreProcess=False):\n",
    "        '''\n",
    "        Initializer function for the CustomDataSetForCNN class\n",
    "        params:\n",
    "        input_seq_length : input sequence length to be considered\n",
    "        output_seq_length : output sequence length to be predicted\n",
    "        datasets : The indices of the datasets to use\n",
    "        test_data_sets : The indices of the test sets from datasets\n",
    "        dev_ratio_to_test_set : ratio of the validation set size to the test set size\n",
    "        forcePreProcess : Flag to forcefully preprocess the data again from csv files\n",
    "        '''\n",
    "        # List of data directories where raw data resides\n",
    "        self.data_dirs = ['./data/train/processed/biwi/biwi_hotel', './data/train/processed/crowds/arxiepiskopi1',\n",
    "                          './data/train/processed/crowds/crowds_zara02', './data/train/processed/crowds/crowds_zara03',\n",
    "                          './data/train/processed/crowds/students001', './data/train/processed/crowds/students003', \n",
    "                          './data/train/processed/stanford/bookstore_0',\n",
    "                          './data/train/processed/stanford/bookstore_1', './data/train/processed/stanford/bookstore_2',\n",
    "                          './data/train/processed/stanford/bookstore_3', './data/train/processed/stanford/coupa_3',\n",
    "                          './data/train/processed/stanford/deathCircle_0', './data/train/processed/stanford/deathCircle_1',\n",
    "                          './data/train/processed/stanford/deathCircle_2', './data/train/processed/stanford/deathCircle_3',\n",
    "                          './data/train/processed/stanford/deathCircle_4', './data/train/processed/stanford/gates_0',\n",
    "                          './data/train/processed/stanford/gates_1', './data/train/processed/stanford/gates_3',\n",
    "                          './data/train/processed/stanford/gates_4', './data/train/processed/stanford/gates_5',\n",
    "                          './data/train/processed/stanford/gates_6', './data/train/processed/stanford/gates_7',\n",
    "                          './data/train/processed/stanford/gates_8', './data/train/processed/stanford/hyang_4',\n",
    "                          './data/train/processed/stanford/hyang_5', './data/train/processed/stanford/hyang_6',\n",
    "                          './data/train/processed/stanford/hyang_7', './data/train/processed/stanford/hyang_9',\n",
    "                          './data/train/processed/stanford/nexus_0', './data/train/processed/stanford/nexus_1',\n",
    "                          './data/train/processed/stanford/nexus_2', './data/train/processed/stanford/nexus_3',\n",
    "                          './data/train/processed/stanford/nexus_4', './data/train/processed/stanford/nexus_7',\n",
    "                          './data/train/processed/stanford/nexus_8', './data/train/processed/stanford/nexus_9']\n",
    "        train_datasets = datasets\n",
    "        for dataset in test_data_sets:\n",
    "            train_datasets.remove(dataset)\n",
    "        self.train_data_dirs = [self.data_dirs[x] for x in train_datasets]\n",
    "        self.test_data_dirs = [self.data_dirs[x] for x in test_data_sets]\n",
    "        \n",
    "        # Number of datasets\n",
    "        self.numDatasets = len(self.data_dirs)\n",
    "        \n",
    "        # Data directory where the pre-processed pickle file resides\n",
    "        self.data_dir = './data/train/processed'\n",
    "        \n",
    "        # Store the arguments\n",
    "        self.input_seq_length = input_seq_length\n",
    "        self.pred_seq_length = pred_seq_length\n",
    "        \n",
    "        # Validation arguments\n",
    "        self.dev_fraction = dev_ratio_to_test_set\n",
    "        \n",
    "        # Buffer for storing processed data.\n",
    "        self.processed_input_output_pairs = []\n",
    "        \n",
    "        # Define the path in which the process data would be stored\n",
    "        self.processed_train_data_file = os.path.join(self.data_dir, \"trajectories_cnn_train.cpkl\")\n",
    "        self.processed_dev_data_file = os.path.join(self.data_dir, \"trajectories_cnn_dev.cpkl\")\n",
    "        self.processed_test_data_file = os.path.join(self.data_dir, \"trajectories_cnn_test.cpkl\")\n",
    "        \n",
    "        # If the file doesn't exist or forcePreProcess is true\n",
    "        if not(os.path.exists(self.processed_train_data_file)) or forcePreProcess:\n",
    "            print(\"============ Creating pre-processed training data for CNN ============\")\n",
    "            self.preprocess(self.train_data_dirs, self.processed_train_data_file)\n",
    "        if not(os.path.exists(self.processed_dev_data_file)) or not(os.path.exists(self.processed_test_data_file)) or forcePreProcess:\n",
    "            print(\"============ Creating pre-processed dev & test data for CNN ============\")\n",
    "            self.preprocess(self.test_data_dirs, self.processed_test_data_file, self.dev_fraction, self.processed_dev_data_file)\n",
    "        \n",
    "    def preprocess(self, data_dirs, data_file, dev_fraction = 0., data_file_2 = None):\n",
    "        random.seed(1) # Random seed for pedestrian permutation and data shuffling\n",
    "        for directory in data_dirs:\n",
    "            print('--> Processing dataset ' + str(directory))\n",
    "            # define path of the csv file of the current dataset\n",
    "            file_path = os.path.join(directory, 'world_pos_normalized.csv')\n",
    "            \n",
    "            # Load the data from the csv file\n",
    "            data = np.genfromtxt(file_path, delimiter=',')\n",
    "            \n",
    "            # Frame IDs of the frames in the current dataset\n",
    "            frameList = np.unique(data[0, :]).tolist()\n",
    "            numFrames = len(frameList)\n",
    "            \n",
    "            # Frame ID increment for this dataset.\n",
    "            frame_increment = np.min(np.array(frameList[1:-1]) - np.array(frameList[0:-2]))\n",
    "            \n",
    "            # For this dataset check which pedestrians exist in each frame.\n",
    "            pedsInFrameList = []\n",
    "            pedsPosInFrameList = []\n",
    "            for ind, frame in enumerate(frameList):\n",
    "                # For this frame check the pedestrian IDs.\n",
    "                pedsInFrame = data[:, data[0, :] == frame]\n",
    "                pedsList = pedsInFrame[1, :].tolist()\n",
    "                pedsInFrameList.append(pedsList)\n",
    "                # Position information for each pedestrian.\n",
    "                pedsPos = []\n",
    "                for ped in pedsList:\n",
    "                    # Extract x and y positions\n",
    "                    current_x = pedsInFrame[2, pedsInFrame[1, :] == ped][0]\n",
    "                    current_y = pedsInFrame[3, pedsInFrame[1, :] == ped][0]\n",
    "                    pedsPos.extend([current_x, current_y])\n",
    "                    if (current_x == 0.0 and current_y == 0.0):\n",
    "                        print('[WARNING] There exists a pedestrian at coordinate [0.0, 0.0]')\n",
    "                pedsPosInFrameList.append(pedsPos)\n",
    "            \n",
    "            # Go over the frames in this data again to extract data.\n",
    "            ind = 0\n",
    "            \n",
    "            while ind < len(frameList) - (self.input_seq_length + self.pred_seq_length):\n",
    "                 # Check if this sequence contains consecutive frames. Otherwise skip this sequence.\n",
    "                if not frameList[ind + self.input_seq_length + self.pred_seq_length] - frameList[ind] == (self.input_seq_length + self.pred_seq_length)*frame_increment:\n",
    "                    ind += 1\n",
    "                    continue;\n",
    "                # List of pedestirans in this sequence.\n",
    "                pedsList = np.unique(np.concatenate(pedsInFrameList[ind : ind + self.input_seq_length + self.pred_seq_length])).tolist()\n",
    "                # Print the Frame numbers and pedestrian IDs in this sequence for sanity check.\n",
    "                # print(str(int(self.input_seq_length + self.pred_seq_length)) + ' frames starting from Frame ' + str(int(frameList[ind])) +  ' contain pedestrians ' + str(pedsList))\n",
    "                # Initialize numpy arrays for input-output pair\n",
    "                data_input = np.zeros((2*len(pedsList), self.input_seq_length))\n",
    "                data_output = np.zeros((2*len(pedsList), self.pred_seq_length))\n",
    "                for ii in range(self.input_seq_length):\n",
    "                    for jj in range(len(pedsList)):\n",
    "                        if pedsList[jj] in pedsInFrameList[ind + ii]:\n",
    "                            datum_index = pedsInFrameList[ind + ii].index(pedsList[jj])\n",
    "                            data_input[2*jj:2*(jj + 1), ii] = np.array(pedsPosInFrameList[ind + ii][2*datum_index:2*(datum_index + 1)])\n",
    "                for ii in range(self.pred_seq_length):\n",
    "                    for jj in range(len(pedsList)):\n",
    "                        if pedsList[jj] in pedsInFrameList[ind + self.input_seq_length + ii]:\n",
    "                            datum_index = pedsInFrameList[ind + self.input_seq_length + ii].index(pedsList[jj])\n",
    "                            data_output[2*jj:2*(jj + 1), ii] = np.array(pedsPosInFrameList[ind + self.input_seq_length + ii][2*datum_index:2*(datum_index + 1)])\n",
    "                processed_pair = (torch.from_numpy(data_input), torch.from_numpy(data_output))\n",
    "                self.processed_input_output_pairs.append(processed_pair)\n",
    "                \n",
    "                ind += self.input_seq_length + self.pred_seq_length\n",
    "        print('--> Original Data Size: ' + str(len(self.processed_input_output_pairs)))   \n",
    "        # Perform data augmentation\n",
    "        self.augment_rotate()\n",
    "        self.augment_flip()\n",
    "        self.augment_permute()\n",
    "            \n",
    "        # Shuffle data, possibly divide into train and dev sets.\n",
    "        print('--> Shuffling all data before saving')\n",
    "        random.shuffle(self.processed_input_output_pairs)\n",
    "        if dev_fraction != 0.:\n",
    "            assert(data_file_2 != None)\n",
    "            dev_size = int(len(self.processed_input_output_pairs)*dev_fraction)\n",
    "            processed_dev_set = self.processed_input_output_pairs[:dev_size]\n",
    "            processed_test_set = self.processed_input_output_pairs[dev_size:]\n",
    "            # Save processed data.\n",
    "            f = open(data_file, 'wb')\n",
    "            print('--> Dumping test data to pickle file')\n",
    "            pickle.dump(processed_test_set, f, protocol=2)\n",
    "            f.close()\n",
    "            f2 = open(data_file_2, 'wb')\n",
    "            print('--> Dumping dev data to pickle file')\n",
    "            pickle.dump(processed_dev_set, f2, protocol=2)\n",
    "            f2.close()\n",
    "            # Clear buffer\n",
    "            self.processed_input_output_pairs = []\n",
    "        else:\n",
    "            # Save processed data.\n",
    "            f = open(data_file, 'wb')\n",
    "            print('--> Dumping training data to pickle file')\n",
    "            pickle.dump(self.processed_input_output_pairs, f, protocol=2)\n",
    "            f.close()\n",
    "            # Clear buffer\n",
    "            self.processed_input_output_pairs = []\n",
    "    \n",
    "    def augment_rotate(self):\n",
    "        deg_increment_int = 30\n",
    "        print('--> Data Augmentation 1: Coordinate Rotation (' + str(deg_increment_int) +' deg increment)')\n",
    "        augmented_input_output_pairs = []\n",
    "        for processed_input_output_pair in tqdm(self.processed_input_output_pairs):\n",
    "            data_input, data_output = processed_input_output_pair[0].numpy(), processed_input_output_pair[1].numpy()\n",
    "            num_peds = int(data_input.shape[0]/2)\n",
    "            # Rotate by deg_increment deg sequentially\n",
    "            for deg in range(deg_increment_int, 360, deg_increment_int):\n",
    "                data_input_rotated = np.zeros_like(data_input)\n",
    "                data_output_rotated = np.zeros_like(data_output)\n",
    "                rad = np.radians(deg)\n",
    "                c, s = np.cos(rad), np.sin(rad)\n",
    "                Rot = np.array(((c,-s), (s, c)))\n",
    "                for ii in range(num_peds):\n",
    "                    for jj in range(self.input_seq_length):\n",
    "                        coordinates_for_this_ped = data_input[2*ii:2*(ii+1), jj]\n",
    "                        new_coordinates_for_this_ped = np.dot(Rot, coordinates_for_this_ped)\n",
    "                        data_input_rotated[2*ii:2*(ii+1), jj] = new_coordinates_for_this_ped\n",
    "                    for jj in range(self.pred_seq_length):\n",
    "                        coordinates_for_this_ped = data_output[2*ii:2*(ii+1), jj]\n",
    "                        new_coordinates_for_this_ped = np.dot(Rot, coordinates_for_this_ped)\n",
    "                        data_output_rotated[2*ii:2*(ii+1), jj] = new_coordinates_for_this_ped\n",
    "                processed_pair_rotated = (torch.from_numpy(data_input_rotated), torch.from_numpy(data_output_rotated))\n",
    "                augmented_input_output_pairs.append(processed_pair_rotated)\n",
    "        self.processed_input_output_pairs.extend(augmented_input_output_pairs)\n",
    "        print('--> Augmented Data Size: ' + str(len(self.processed_input_output_pairs)))\n",
    "    \n",
    "    def augment_flip(self):\n",
    "        print('--> Data Augmentation 2: Y Flip')\n",
    "        augmented_input_output_pairs = []\n",
    "        for processed_input_output_pair in tqdm(self.processed_input_output_pairs):\n",
    "            data_input, data_output = processed_input_output_pair[0].numpy(), processed_input_output_pair[1].numpy()\n",
    "            num_peds = int(data_input.shape[0]/2)\n",
    "            # Flip y\n",
    "            data_input_yflipped = np.zeros_like(data_input)\n",
    "            data_output_yflipped = np.zeros_like(data_output)\n",
    "            for kk in range(num_peds):\n",
    "                data_input_yflipped[2*kk, :] = data_input[2*kk, :]\n",
    "                data_input_yflipped[2*kk+1, :] = -1*data_input[2*kk+1, :]\n",
    "                data_output_yflipped[2*kk, :] = data_output[2*kk, :]\n",
    "                data_output_yflipped[2*kk+1, :] = -1*data_output[2*kk+1, :]\n",
    "            processed_pair_yflipped = (torch.from_numpy(data_input_yflipped), torch.from_numpy(data_output_yflipped))\n",
    "            augmented_input_output_pairs.append(processed_pair_yflipped)\n",
    "        self.processed_input_output_pairs.extend(augmented_input_output_pairs)\n",
    "        print('--> Augmented Data Size: ' + str(len(self.processed_input_output_pairs)))\n",
    "        \n",
    "    def augment_permute(self):\n",
    "        # Specify how many pedestrian permutations to consider per input-output pair\n",
    "        num_perms = 4\n",
    "        print('--> Data Augmentation 3: Pedestrian Permutation (' + str(num_perms) + ' random permutations per input-output pair)')\n",
    "        augmented_input_output_pairs = []\n",
    "        for processed_input_output_pair in tqdm(self.processed_input_output_pairs):\n",
    "            data_input, data_output = processed_input_output_pair[0].numpy(), processed_input_output_pair[1].numpy()\n",
    "            num_peds = int(data_input.shape[0]/2)\n",
    "            for ii in range(num_perms):\n",
    "                perm = np.random.permutation(num_peds)\n",
    "                data_input_permuted = np.zeros_like(data_input)\n",
    "                data_output_permuted = np.zeros_like(data_output)\n",
    "                for jj in range(len(perm)):\n",
    "                    data_input_permuted[2*jj:2*(jj+1), :] = data_input[2*perm[jj]:2*(perm[jj]+1), :]\n",
    "                    data_output_permuted[2*jj:2*(jj+1), :] = data_output[2*perm[jj]:2*(perm[jj]+1), :]\n",
    "                processed_pair_permuted = (torch.from_numpy(data_input_permuted), torch.from_numpy(data_output_permuted))\n",
    "                augmented_input_output_pairs.append(processed_pair_permuted)\n",
    "        self.processed_input_output_pairs.extend(augmented_input_output_pairs)\n",
    "        print('--> Augmented Data Size: ' + str(len(self.processed_input_output_pairs)))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Creating pre-processed training data for CNN ============\n",
      "--> Processing dataset ./data/train/processed/biwi/biwi_hotel\n",
      "--> Processing dataset ./data/train/processed/crowds/arxiepiskopi1\n",
      "--> Processing dataset ./data/train/processed/crowds/crowds_zara03\n",
      "--> Processing dataset ./data/train/processed/crowds/students001\n",
      "--> Processing dataset ./data/train/processed/crowds/students003\n",
      "--> Processing dataset ./data/train/processed/stanford/bookstore_0\n",
      "--> Processing dataset ./data/train/processed/stanford/bookstore_1\n",
      "--> Processing dataset ./data/train/processed/stanford/bookstore_2\n",
      "--> Processing dataset ./data/train/processed/stanford/bookstore_3\n",
      "--> Processing dataset ./data/train/processed/stanford/coupa_3\n",
      "--> Processing dataset ./data/train/processed/stanford/deathCircle_0\n",
      "--> Processing dataset ./data/train/processed/stanford/deathCircle_1\n",
      "--> Processing dataset ./data/train/processed/stanford/deathCircle_2\n",
      "--> Processing dataset ./data/train/processed/stanford/deathCircle_3\n",
      "--> Processing dataset ./data/train/processed/stanford/deathCircle_4\n",
      "--> Processing dataset ./data/train/processed/stanford/gates_0\n",
      "--> Processing dataset ./data/train/processed/stanford/gates_1\n",
      "--> Processing dataset ./data/train/processed/stanford/gates_3\n",
      "--> Processing dataset ./data/train/processed/stanford/gates_4\n",
      "--> Processing dataset ./data/train/processed/stanford/gates_5\n",
      "--> Processing dataset ./data/train/processed/stanford/gates_6\n",
      "--> Processing dataset ./data/train/processed/stanford/gates_7\n",
      "--> Processing dataset ./data/train/processed/stanford/gates_8\n",
      "--> Processing dataset ./data/train/processed/stanford/hyang_4\n",
      "--> Processing dataset ./data/train/processed/stanford/hyang_5\n",
      "--> Processing dataset ./data/train/processed/stanford/hyang_6\n",
      "--> Processing dataset ./data/train/processed/stanford/hyang_7\n",
      "--> Processing dataset ./data/train/processed/stanford/hyang_9\n",
      "--> Processing dataset ./data/train/processed/stanford/nexus_0\n",
      "--> Processing dataset ./data/train/processed/stanford/nexus_1\n",
      "--> Processing dataset ./data/train/processed/stanford/nexus_2\n",
      "--> Processing dataset ./data/train/processed/stanford/nexus_3\n",
      "--> Processing dataset ./data/train/processed/stanford/nexus_4\n",
      "--> Processing dataset ./data/train/processed/stanford/nexus_7\n",
      "--> Processing dataset ./data/train/processed/stanford/nexus_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/2273 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Processing dataset ./data/train/processed/stanford/nexus_9\n",
      "--> Original Data Size: 2273\n",
      "--> Data Augmentation 1: Coordinate Rotation (30 deg increment)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  4%|▍         | 99/2273 [00:00<00:02, 985.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 154/2273 [00:00<00:02, 785.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 191/2273 [00:00<00:06, 314.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 221/2273 [00:00<00:08, 249.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 249/2273 [00:00<00:07, 257.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 286/2273 [00:00<00:07, 282.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 318/2273 [00:00<00:06, 289.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 353/2273 [00:01<00:06, 305.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 409/2273 [00:01<00:05, 352.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 464/2273 [00:01<00:04, 393.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 525/2273 [00:01<00:03, 439.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 589/2273 [00:01<00:03, 484.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 658/2273 [00:01<00:03, 529.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 719/2273 [00:01<00:02, 548.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 778/2273 [00:01<00:03, 493.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 831/2273 [00:01<00:03, 459.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▊      | 880/2273 [00:02<00:03, 422.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 935/2273 [00:02<00:02, 451.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 983/2273 [00:02<00:02, 431.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 1028/2273 [00:02<00:03, 370.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 1075/2273 [00:02<00:03, 395.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 1148/2273 [00:02<00:02, 457.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 1242/2273 [00:02<00:01, 540.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 1306/2273 [00:02<00:01, 527.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 1380/2273 [00:03<00:01, 576.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 1505/2273 [00:03<00:01, 686.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 1587/2273 [00:03<00:01, 635.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 1661/2273 [00:03<00:00, 614.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 1730/2273 [00:03<00:00, 600.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 1797/2273 [00:03<00:00, 616.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 1863/2273 [00:03<00:00, 505.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 1920/2273 [00:03<00:00, 498.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 1975/2273 [00:04<00:00, 477.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 2032/2273 [00:04<00:00, 498.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 2104/2273 [00:04<00:00, 549.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 2164/2273 [00:04<00:00, 563.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 2233/2273 [00:04<00:00, 595.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 2273/2273 [00:04<00:00, 503.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/27276 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 1624/27276 [00:00<00:01, 16236.45it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Augmented Data Size: 27276\n",
      "--> Data Augmentation 2: Y Flip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3948/27276 [00:00<00:01, 17848.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 4986/27276 [00:00<00:02, 11030.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 6193/27276 [00:00<00:01, 11322.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 8068/27276 [00:00<00:01, 12848.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 10010/27276 [00:00<00:01, 14299.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 11473/27276 [00:00<00:01, 14064.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 12958/27276 [00:00<00:01, 14290.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 14405/27276 [00:00<00:00, 13822.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 16672/27276 [00:01<00:00, 15654.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 19253/27276 [00:01<00:00, 17748.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 21637/27276 [00:01<00:00, 19220.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 23707/27276 [00:01<00:00, 17537.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 25652/27276 [00:01<00:00, 18070.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 27276/27276 [00:01<00:00, 16901.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/54552 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 187/54552 [00:00<00:29, 1847.72it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Augmented Data Size: 54552\n",
      "--> Data Augmentation 3: Pedestrian Permutation (10 random permutations per input-output pair)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 270/54552 [00:00<00:47, 1148.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 465/54552 [00:00<00:41, 1308.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 724/54552 [00:00<00:35, 1535.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 914/54552 [00:00<00:32, 1628.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 1106/54552 [00:00<00:31, 1706.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1389/54552 [00:00<00:27, 1936.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1670/54552 [00:00<00:24, 2135.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1897/54552 [00:00<00:24, 2164.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 2134/54552 [00:01<00:23, 2220.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 2468/54552 [00:01<00:21, 2467.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 2810/54552 [00:01<00:19, 2692.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 3095/54552 [00:01<00:18, 2725.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 3379/54552 [00:01<00:18, 2701.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 3720/54552 [00:01<00:17, 2879.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 4017/54552 [00:01<00:23, 2160.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 4266/54552 [00:02<00:39, 1285.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 4460/54552 [00:02<00:44, 1127.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 4622/54552 [00:02<00:50, 995.79it/s] \u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 4759/54552 [00:02<00:52, 947.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 4881/54552 [00:02<00:57, 867.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 5055/54552 [00:03<00:48, 1020.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 5246/54552 [00:03<00:41, 1184.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 5392/54552 [00:03<00:39, 1255.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a5a245eca2df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataPreprocessorForCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforcePreProcess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_ratio_to_test_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-d0a0187bceba>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_seq_length, pred_seq_length, datasets, test_data_sets, dev_ratio_to_test_set, forcePreProcess)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_train_data_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mforcePreProcess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"============ Creating pre-processed training data for CNN ============\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data_dirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_train_data_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_dev_data_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_test_data_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mforcePreProcess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"============ Creating pre-processed dev & test data for CNN ============\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-d0a0187bceba>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, data_dirs, data_file, dev_fraction, data_file_2)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_rotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_flip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_permute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Shuffle data, possibly divide into train and dev sets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-d0a0187bceba>\u001b[0m in \u001b[0;36maugment_permute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_perms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_peds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mdata_input_permuted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0mdata_output_permuted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mjj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pedestrian_prediction/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mzeros_like\u001b[0;34m(a, dtype, order, subok)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;31m# needed instead of a 0 to get same result as zeros for for string dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unsafe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "processed = CustomDataPreprocessorForCNN(forcePreProcess=True, test_data_sets=[2], dev_ratio_to_test_set=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open(processed.processed_train_data_file, 'rb')\n",
    "dev_file = open(processed.processed_dev_data_file, 'rb')\n",
    "test_file = open(processed.processed_test_data_file, 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/train/processed/trajectories_cnn_train.cpkl'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.processed_train_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(train_file)\n",
    "dev = pickle.load(dev_file)\n",
    "test = pickle.load(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2273"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetForCNN(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.file = open(self.file_path, 'rb')\n",
    "        self.data = pickle.load(self.file)\n",
    "        self.file.close()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDatasetForCNN(processed.processed_train_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_set.__getitem__(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1771, 0.1612, 0.1471, 0.1313, 0.1158],\n",
       "        [0.6733, 0.6807, 0.6873, 0.6947, 0.6982]], dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.1777,  0.1773,  0.1773,  0.1773,  0.1773],\n",
       "          [ 0.3633,  0.3771,  0.3907,  0.4044,  0.4180],\n",
       "          [ 0.1191,  0.1191,  0.1191,  0.1191,  0.1191],\n",
       "          [ 0.0447,  0.0447,  0.0447,  0.0447,  0.0447],\n",
       "          [ 0.0983,  0.0983,  0.0983,  0.0983,  0.0983],\n",
       "          [ 0.0544,  0.0544,  0.0544,  0.0544,  0.0544],\n",
       "          [ 0.0721,  0.0721,  0.0721,  0.0721,  0.0721],\n",
       "          [ 0.0598,  0.0598,  0.0598,  0.0598,  0.0598],\n",
       "          [ 0.0661,  0.0661,  0.0661,  0.0661,  0.0661],\n",
       "          [ 0.0401,  0.0401,  0.0401,  0.0401,  0.0401],\n",
       "          [-0.3741, -0.3772, -0.3802, -0.3820, -0.3850],\n",
       "          [ 0.7355,  0.7404,  0.7452,  0.7528,  0.7576],\n",
       "          [ 0.1136,  0.1219,  0.1327,  0.1418,  0.1506],\n",
       "          [ 0.2656,  0.2787,  0.2928,  0.3077,  0.3223],\n",
       "          [-0.1503, -0.1503, -0.1503, -0.1503, -0.1503],\n",
       "          [ 0.2184,  0.2184,  0.2184,  0.2184,  0.2184],\n",
       "          [ 0.1071,  0.1047,  0.1019,  0.0973,  0.0945],\n",
       "          [ 0.1414,  0.1318,  0.1213,  0.1115,  0.1009],\n",
       "          [-0.3563, -0.3442, -0.3333, -0.3211, -0.3089],\n",
       "          [ 0.3558,  0.3501,  0.3450,  0.3393,  0.3337],\n",
       "          [-0.3431, -0.3309, -0.3183, -0.3079, -0.2957],\n",
       "          [ 0.3773,  0.3716,  0.3657,  0.3608,  0.3552],\n",
       "          [ 0.5520,  0.5376,  0.5229,  0.5083,  0.4938],\n",
       "          [-0.0797, -0.0730, -0.0683, -0.0632, -0.0581],\n",
       "          [ 0.5622,  0.5479,  0.5339,  0.5187,  0.5048],\n",
       "          [-0.0624, -0.0557, -0.0492, -0.0455, -0.0390],\n",
       "          [-0.1782, -0.1782, -0.1782, -0.1782, -0.1782],\n",
       "          [ 0.2450,  0.2450,  0.2450,  0.2450,  0.2450],\n",
       "          [-0.2274, -0.2204, -0.2118, -0.2031, -0.1961],\n",
       "          [ 0.6118,  0.6086,  0.6079,  0.6038,  0.6006],\n",
       "          [ 0.1273,  0.1273,  0.1273,  0.1273,  0.1273],\n",
       "          [ 0.3936,  0.3936,  0.3936,  0.3936,  0.3936],\n",
       "          [ 0.0215,  0.0267,  0.0336,  0.0390,  0.0456],\n",
       "          [ 0.1281,  0.1393,  0.1497,  0.1612,  0.1718],\n",
       "          [ 0.0363,  0.0432,  0.0502,  0.0573,  0.0642],\n",
       "          [ 0.1178,  0.1282,  0.1385,  0.1493,  0.1596],\n",
       "          [-0.1054, -0.1002, -0.0963, -0.0911, -0.0871],\n",
       "          [-0.1497, -0.1384, -0.1301, -0.1190, -0.1105],\n",
       "          [ 0.6444,  0.6305,  0.6162,  0.6005,  0.5866],\n",
       "          [-0.0075, -0.0010,  0.0057,  0.0130,  0.0195],\n",
       "          [ 0.0957,  0.0957,  0.0957,  0.0957,  0.0957],\n",
       "          [ 0.4645,  0.4645,  0.4645,  0.4645,  0.4645],\n",
       "          [-0.7535, -0.7592, -0.7636, -0.7692, -0.7735],\n",
       "          [-0.3529, -0.3469, -0.3415, -0.3354, -0.3266]]], dtype=torch.float64),\n",
       " tensor([[[ 0.1773,  0.1708,  0.1627,  0.1535,  0.1446],\n",
       "          [ 0.4180,  0.4312,  0.4422,  0.4499,  0.4579],\n",
       "          [ 0.1191,  0.1191,  0.1191,  0.1191,  0.1191],\n",
       "          [ 0.0447,  0.0447,  0.0447,  0.0447,  0.0447],\n",
       "          [ 0.0983,  0.0983,  0.0983,  0.0983,  0.0983],\n",
       "          [ 0.0544,  0.0544,  0.0544,  0.0544,  0.0544],\n",
       "          [ 0.0721,  0.0721,  0.0721,  0.0721,  0.0721],\n",
       "          [ 0.0598,  0.0598,  0.0598,  0.0598,  0.0598],\n",
       "          [ 0.0661,  0.0661,  0.0661,  0.0661,  0.0661],\n",
       "          [ 0.0401,  0.0401,  0.0401,  0.0401,  0.0401],\n",
       "          [-0.3850, -0.3880, -0.3902, -0.3928, -0.3959],\n",
       "          [ 0.7576,  0.7624,  0.7669,  0.7749,  0.7797],\n",
       "          [ 0.1506,  0.1589,  0.1667,  0.1731,  0.1780],\n",
       "          [ 0.3223,  0.3355,  0.3523,  0.3706,  0.3853],\n",
       "          [-0.1503, -0.1503, -0.1503, -0.1503, -0.1503],\n",
       "          [ 0.2184,  0.2184,  0.2184,  0.2184,  0.2184],\n",
       "          [ 0.0945,  0.0906,  0.0841,  0.0745,  0.0667],\n",
       "          [ 0.1009,  0.0925,  0.0921,  0.0898,  0.0866],\n",
       "          [-0.3089, -0.2981, -0.2859, -0.2724, -0.2619],\n",
       "          [ 0.3337,  0.3286,  0.3229,  0.3200,  0.3152],\n",
       "          [-0.2957, -0.2831, -0.2727, -0.2605, -0.2483],\n",
       "          [ 0.3552,  0.3493,  0.3444,  0.3387,  0.3331],\n",
       "          [ 0.4938,  0.4799,  0.4659,  0.4507,  0.4364],\n",
       "          [-0.0581, -0.0516, -0.0451, -0.0414, -0.0347],\n",
       "          [ 0.5048,  0.4926,  0.4774,  0.4635,  0.4491],\n",
       "          [-0.0390, -0.0333, -0.0296, -0.0231, -0.0164],\n",
       "          [-0.1782, -0.1782, -0.1782, -0.1782, -0.1782],\n",
       "          [ 0.2450,  0.2450,  0.2450,  0.2450,  0.2450],\n",
       "          [-0.1961, -0.1877, -0.1807, -0.1716, -0.1633],\n",
       "          [ 0.6006,  0.6005,  0.5973,  0.5930,  0.5925],\n",
       "          [ 0.1273,  0.1273,  0.1273,  0.1256,  0.1256],\n",
       "          [ 0.3936,  0.3936,  0.3936,  0.3944,  0.3944],\n",
       "          [ 0.0456,  0.0518,  0.0579,  0.0631,  0.0679],\n",
       "          [ 0.1718,  0.1829,  0.1937,  0.2049,  0.2128],\n",
       "          [ 0.0642,  0.0712,  0.0783,  0.0853,  0.0923],\n",
       "          [ 0.1596,  0.1700,  0.1807,  0.1911,  0.2015],\n",
       "          [-0.0871, -0.0816, -0.0777, -0.0725, -0.0685],\n",
       "          [-0.1105, -0.0987, -0.0902, -0.0791, -0.0707],\n",
       "          [ 0.5866,  0.5723,  0.5566,  0.5426,  0.5288],\n",
       "          [ 0.0195,  0.0262,  0.0335,  0.0400,  0.0464],\n",
       "          [ 0.0957,  0.0957,  0.0957,  0.0957,  0.0957],\n",
       "          [ 0.4645,  0.4645,  0.4645,  0.4645,  0.4645],\n",
       "          [-0.7735, -0.7766, -0.7814, -0.7844, -0.7905],\n",
       "          [-0.3266, -0.3149, -0.3025, -0.2908, -0.2813]]], dtype=torch.float64)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_set.__getitem__(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56592"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
